{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOagbNlos0wZtYPHhAwGB13",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/baminro/Dataset-/blob/main/FOR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmfP4VL03qLb"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install tensorflow[and-cuda]\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt # Visualization\n",
        "import matplotlib.dates as mdates # Formatting dates\n",
        "import seaborn as sns # Visualization\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import torch # Library for implementing Deep Neural Network\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "\n",
        "data=pd.read_excel('/content/drive/MyDrive/copper.xlsx')\n",
        "data=data.set_index('Time')\n",
        "data['SMA30Var1'] = data['Var1'].rolling(252).mean()\n",
        "data['SMA5Var1'] = data['Var1'].rolling(100).mean()\n",
        "data['SMA5Var1-SMA30Var1']=data['SMA5Var1']-data['SMA30Var1']\n",
        "data=data[['Var1']]\n",
        "data.dropna(inplace=True)\n",
        "data\n",
        "\n",
        "\n",
        "# Normalize the data\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "# Split the data into train and test sets\n",
        "train_size = int(len(scaled_data) * 0.8)\n",
        "train_data = scaled_data[:train_size]\n",
        "test_data = scaled_data[train_size:]\n",
        "\n",
        "\n",
        "def create_sequences(data, seq_length):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length-252):\n",
        "        X.append(data[i:i+seq_length])\n",
        "        y.append(data[i+seq_length+1:i+seq_length+252+1])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "seq_length =252  # Number of time steps to look back\n",
        "X_train, y_train = create_sequences(train_data, seq_length)\n",
        "X_test, y_test = create_sequences(test_data, seq_length)\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input,Conv1D, Dense,Add, LayerNormalization, MultiHeadAttention, Dropout, GlobalAveragePooling1D\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "# Transformer Block\n",
        "\n",
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim,\n",
        "                        dropout=0, attention_axes=None):\n",
        "\n",
        "  x = LayerNormalization(epsilon=0.001)(inputs)\n",
        "  x = MultiHeadAttention(\n",
        "      key_dim=head_size, num_heads=num_heads, dropout=dropout,\n",
        "      attention_axes=attention_axes\n",
        "      )(x, x)\n",
        "  x = Dropout(dropout)(x)\n",
        "  res = x + inputs\n",
        "\n",
        "    # Feed Forward Part\n",
        "  x = LayerNormalization(epsilon=0.001)(res)\n",
        "  x = Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
        "  x = Dropout(dropout)(x)\n",
        "  x = Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
        "  return x + res\n",
        "\n",
        "def build_transfromer(head_size,\n",
        "                      num_heads,\n",
        "                      ff_dim,\n",
        "                      num_trans_blocks,\n",
        "                      mlp_units, dropout=0, mlp_dropout=0) -> tf.keras.Model:\n",
        "\n",
        "  n_timesteps, n_features, n_outputs = 252, 1, 1\n",
        "  inputs = tf.keras.Input(shape=(n_timesteps, n_features))\n",
        "  x = inputs\n",
        "  for _ in range(num_trans_blocks):\n",
        "    x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
        "\n",
        "  x = GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
        "  for dim in mlp_units:\n",
        "    x = Dense(dim, activation=\"relu\")(x)\n",
        "\n",
        "    x = Dropout(mlp_dropout)(x)\n",
        "  x=keras.layers.Reshape((n_timesteps,1))(x)\n",
        "  outputs = Dense(1, activation='relu')(x)\n",
        " # tf.keras.layers(outputs,(n_timesteps,1))\n",
        "\n",
        "  return tf.keras.Model(inputs, outputs)\n",
        "\n",
        "transformer = build_transfromer(head_size=75, num_heads=2, ff_dim=2,\n",
        "                                num_trans_blocks=2, mlp_units=[252],\n",
        "                                mlp_dropout=0.20, dropout=0.20,\n",
        "                              )\n",
        "transformer.summary()\n",
        "transformer.compile(\n",
        "    loss=\"mse\",\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "    metrics=[\"mse\"],\n",
        ")\n",
        "\n",
        "filepath = 'tf1_mnist_cnn.weights.h5'\n",
        "save_checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1,\n",
        "                             save_best_only=True, save_weights_only=True,\n",
        "                             mode='auto',  save_freq=\"epoch\",)\n",
        "\n",
        "history = transformer.fit(X_train, y_train[:,:,0], batch_size=12,\n",
        "                         epochs=10, validation_data=(X_test, y_test[:,:,0]),\n",
        "                         verbose=1,shuffle=True, callbacks=save_checkpoint)\n",
        "\n",
        "# Make predictions\n",
        "transformer.load_weights('tf1_mnist_cnn.weights.h5')\n",
        "train_predict = transformer.predict(X_train)\n",
        "test_predict = transformer.predict(X_test)\n",
        "\n",
        "# Inverse transform predictions\n",
        "#train_predict = scaler.inverse_transform(train_predict)\n",
        "#test_predict = scaler.inverse_transform(test_predict\n",
        "\n",
        "train_mse=mean_squared_error(y_train[:,:,0], train_predict.squeeze())\n",
        "test_mse=mean_squared_error(y_test[:,:,0], test_predict.squeeze())\n",
        "\n",
        "# Evaluate the model (Optional: Calculate RMSE or other metrics)\n",
        "#train_rmse = math.sqrt(mean_squared_error(y_train, scaler.inverse_transform(train_predict.reshape(-1, 1))))\n",
        "#test_rmse = math.sqrt(mean_squared_error(y_test, scaler.inverse_transform(test_predict.reshape(-1, 1))))\n",
        "\n",
        "print(f\"Train MSE: {train_mse}\")\n",
        "print(f\"Test MSE: {test_mse}\")\n",
        "history\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs=epochs = range(1, len(val_loss) + 1)\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'r', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Test loss')\n",
        "plt.title('Training and test loss')\n",
        "plt.title\n",
        "plt.legend()\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "pre=transformer.predict(test_data[-252:,:].reshape(1, 252, 1))\n",
        "forecast_period =252\n",
        "scaler1 = MinMaxScaler()\n",
        "scaled_data = scaler1.fit_transform(np.array(data['Var1'].values.reshape(-1, 1)))\n",
        "forecast = scaler1.inverse_transform(np.array(pre).reshape(-1, 1))\n",
        "forecast\n",
        "\n",
        "forecast=forecast[:]\n",
        "vb=scaler.inverse_transform(test_data[-252:,:])[-1][0]\n",
        "forecast[0]=vb\n",
        "#vb=vb[-1,-1]\n",
        "if (forecast[0]/vb)-1>0.05:\n",
        "    forecast[0]=vb*(1+0.05)\n",
        "if (forecast[0]/vb)-1<-0.05:\n",
        "    forecast[0]=vb*(1-0.05)\n",
        "for i in range(len(forecast)-1):\n",
        "     if (forecast[i+1]/forecast[i])-1>0.05:\n",
        "         forecast[i+1]=forecast[i]*(1+0.05)\n",
        "for i in range(len(forecast)-1):\n",
        "     if (forecast[i+1]/forecast[i])-1<-0.05:\n",
        "         forecast[i+1]=forecast[i]*(1-0.05)\n",
        "\n",
        "import datetime\n",
        "plt.figure(figsize=(10, 6))\n",
        "#plt.plot(data.index[-len(test_data):], scaler1.inverse_transform(test_data)[:,0], label='Actual')\n",
        "plt.plot(pd.date_range(start=data.index[-1]+ datetime.timedelta(days=1), periods=forecast_period, freq='D'), forecast, label='Forecast')\n",
        "plt.title('Copper Time Series Forecasting (252-day Forecast)')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Copper')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "import numpy\n",
        "import pandas as pd\n",
        "ret=pd.DataFrame(forecast)/pd.DataFrame(forecast).shift(1)\n",
        "numpy.std(ret)\n",
        "\n",
        "pd.DataFrame(forecast).to_excel('/content/drive/MyDrive/forcop.xlsx')\n"
      ]
    }
  ]
}